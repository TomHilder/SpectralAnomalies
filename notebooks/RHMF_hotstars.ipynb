{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea1eada-579f-4edd-8b72-167b6d89643d",
   "metadata": {},
   "source": [
    "# Robust HMF on *SDSS-V BOSS* spectra of hot stars...\n",
    "...to find evidence of H-alpha emission.\n",
    "\n",
    "## Authors:\n",
    "- **David W. Hogg** (NYU) (MPIA) (Flatiron)\n",
    "\n",
    "## Contributors and acknowledgements:\n",
    "- **Kareem El-Badry** (Caltech)\n",
    "- **Johanna Müller-Horn** (MPIA)\n",
    "- **Hans-Walter Rix** (MPIA)\n",
    "- **Jaime Villaseñor** (MPIA)\n",
    "- **Eleonora Zari** (Firenze)\n",
    "\n",
    "## Comments:\n",
    "- The `rank` and `nsigma` inputs are currently set by *vibes*.\n",
    "- The iteration to remove H-alpha-weird stars is not necessarily stable or correct. \n",
    "\n",
    "## To-do items:\n",
    "- Maybe switch to a mode where we train on *all* stars of a certain color, and test on yso candidates.\n",
    "- Output a full table of EWs for all stars.\n",
    "- Make some method (perhaps in `rhmf.py`) to save and restore a Robust HMF model.\n",
    "- Fit Voigt profiles to emission lines maybe?\n",
    "- Add more emission lines; see emails from various.\n",
    "\n",
    "## Bugs:\n",
    "- Test time and EW measurement parallelizations aren't fast because of shared-stuff issues, maybe?\n",
    "- Identify and eliminate wrong Doppler shifts and bad wavelength solutions from the training sets?\n",
    "- Plots show Bohr wavelengths, not correct NIST wavelengths, for the Hydrogen lines.\n",
    "- This needs a method to save a model and pick up where it left off.\n",
    "- Maybe RHMF is the wrong tool for this job?\n",
    "- Inconsistent variable names; inconsistent uses of underscores, directory names, file names, etc.\n",
    "- At test time and plotting, produces way too much logging output on the terminal (stdout).\n",
    "- Maybe should think about `zorder` inputs to plotting calls?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from astropy.io import fits\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import os\n",
    "import concurrent.futures\n",
    "import matplotlib.pyplot as plt\n",
    "import rhmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b1821e-d4d7-43ea-b1d0-5495ef2e6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "\n",
    "# this MUST have the strongest emission lines first.\n",
    "LINELIST = [(\"H-alpha\", 6564.6),\n",
    "            (\"H-beta\", 4861.36),\n",
    "            # (\"H-gamma\", 4340.46),\n",
    "            (\"He II 4201\", 4201.015),\n",
    "            (\"He II 4543\", 4542.864),\n",
    "            (\"He II 4687\", 4687.02)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c5e629-4385-4bc5-9e54-f322c7bed0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data choices\n",
    "bosstag = 'v6_2_1'\n",
    "cachedir = f'./boss_{bosstag}_star_cache'\n",
    "os.makedirs(cachedir, exist_ok=True)\n",
    "\n",
    "# Create subdirectory for plots\n",
    "plot_folder = cachedir + '/plots'\n",
    "os.makedirs(plot_folder, exist_ok=True)\n",
    "\n",
    "# Tiny weird github / overleaf interaction hack\n",
    "_ = os.system(\"chmod a+x ./make_pdf.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff5044-fdf2-454a-a415-3222f084107f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model choices\n",
    "rank, nsigma = 24, 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6644a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define download functions\n",
    "\n",
    "# bad global variables! DELETE THESE FOR GITHUB\n",
    "user, password = None, None\n",
    "\n",
    "def download_one_file_from_df(args):\n",
    "    \"\"\"Download a single file from SDSS.\"\"\"\n",
    "    url, filename, user, password, cachedir = args\n",
    "    subdir = filename.split(\"-\")[1]\n",
    "    old_filepath = cachedir + \"/\" + filename\n",
    "    filepath = cachedir + \"/\" + subdir + \"/\" + filename\n",
    "    os.makedirs(cachedir + \"/\" + subdir, exist_ok=True)\n",
    "    \n",
    "    # Skip if already downloaded\n",
    "    if os.path.exists(old_filepath):\n",
    "        os.system(f\"mv -fv {old_filepath} {filepath}\")\n",
    "    if os.path.exists(filepath):\n",
    "        # print(f\"File {filepath} already exists, skipping\")\n",
    "        return filepath\n",
    "\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            response = session.get(url, auth=HTTPBasicAuth(user, password), timeout=30)\n",
    "            response.raise_for_status()\n",
    "            with open(filepath, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        if np.random.uniform() < 0.02: # one in fifty\n",
    "            print(f\"Random example: File downloaded: {filepath}\")\n",
    "        return filepath\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to download {filepath}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def download_files_from_df(df, user, password, dest_folder, boss_tag='v6_2_1', coadd_version='daily', max_workers=16):\n",
    "    \"\"\"Download multiple files from SDSS based on dataframe.\"\"\"\n",
    "    os.makedirs(dest_folder, exist_ok=True)\n",
    "    args_list = []\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        spec_file = row['SPEC_FILE']\n",
    "        fieldid = f\"{row['FIELD']:06d}\"\n",
    "        mjd = str(row['MJD'])\n",
    "        fieldidXXX = fieldid[:-3] + 'XXX'\n",
    "        url = (\n",
    "            f\"https://data.sdss5.org/sas/sdsswork/bhm/boss/spectro/redux/\"\n",
    "            f\"{boss_tag}/spectra/{coadd_version}/lite/{fieldidXXX}/{fieldid}/{mjd}/{spec_file}\"\n",
    "        )\n",
    "        args_list.append((url, spec_file, user, password, dest_folder))\n",
    "\n",
    "    print(f\"Starting attempts to download {len(args_list)} files\")\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = list(executor.map(download_one_file_from_df, args_list))\n",
    "    print(f\"Number successful: {sum([r is not None for r in results])} files\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba33d8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and examine the spAll file\n",
    "spallname = f'spAll-lite-{bosstag}.fits'\n",
    "summaryurl = f'https://data.sdss5.org/sas/sdsswork/bhm/boss/spectro/redux/{bosstag}/summary/daily/{spallname}.gz'\n",
    "summaryfile = cachedir + '/' + spallname + '.gz'\n",
    "summaryfile_uncompressed = cachedir + '/' + spallname\n",
    "\n",
    "if not os.path.exists(summaryfile_uncompressed):\n",
    "    if not os.path.exists(summaryfile):\n",
    "        print(f\"Downloading summary file from {summaryurl}\")\n",
    "        response = requests.get(summaryurl, auth=HTTPBasicAuth(user, password))\n",
    "        with open(summary_file, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Summary file {summaryfile} downloaded\")\n",
    "    \n",
    "    # Decompress\n",
    "    os.system(f'gunzip -v {summaryfile}')\n",
    "    print(f\"Summary file {summaryfile} decompressed\")\n",
    "else:\n",
    "    print(f\"Summary file {summaryfile_uncompressed} already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33fe93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spAll data\n",
    "with fits.open(summaryfile_uncompressed) as hdul:\n",
    "    data = hdul[1].data\n",
    "if False:\n",
    "    print(\"=\"*70)\n",
    "    print(\"ALL AVAILABLE COLUMNS IN SUMMARY FILE\")\n",
    "    print(\"=\"*70)\n",
    "    columns = data.columns.names\n",
    "    for i, col in enumerate(columns):\n",
    "        print(f\"{i+1:3d}. {col}\")\n",
    "print(f\"rows: {len(data)}; columns: {len(data.columns.names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ddad07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a sample of spectra to download\n",
    "snrcut, mwm_ob, mwm_yso = 29., True, False\n",
    "df = pd.DataFrame({col: data[col].byteswap().newbyteorder() for col in ['SPEC_FILE', 'FIELD', 'MJD',\n",
    "                                                                        'SN_MEDIAN_ALL',\n",
    "                                                                        'BP_MAG', 'RP_MAG', \n",
    "                                                                        'FIRSTCARTON', 'PROGRAMNAME']})\n",
    "if mwm_ob:\n",
    "    high_snr_df = df[(df['SN_MEDIAN_ALL'] > snrcut)\n",
    "                   & (df['PROGRAMNAME'] == 'mwm_ob')]\n",
    "if mwm_yso:\n",
    "    high_snr_df = df[(df['SN_MEDIAN_ALL'] > snrcut)\n",
    "                   & (  (df['FIRSTCARTON'] == 'mwm_yso_cluster')\n",
    "                      | (df['FIRSTCARTON'] == 'mwm_yso_s1')\n",
    "                      | (df['FIRSTCARTON'] == 'mwm_yso_s2')\n",
    "                      | (df['FIRSTCARTON'] == 'mwm_yso_s3'))]\n",
    "print(f\"Selected {len(high_snr_df)} spectra\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164f6093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the sample spectra\n",
    "pathnames = download_files_from_df(high_snr_df, user, password, cachedir, boss_tag=bosstag, coadd_version='daily', max_workers=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2aeeef-3e19-4253-8865-37d5301b4e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make lists of strings\n",
    "pathnames = np.array(pathnames)\n",
    "pathnames = pathnames[pathnames != \"\"]\n",
    "filenames = np.array([p.split(\"/\")[-1] for p in pathnames])\n",
    "specnames = np.array([f[5:-5] for f in filenames])\n",
    "print(pathnames.shape, filenames.shape, specnames.shape)\n",
    "print(pathnames[13], filenames[13], specnames[13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0e867e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make rectangular data, plus wavelength grid\n",
    "wavelength = None\n",
    "N = len(filenames)\n",
    "print(f\"reading {N} files...\")\n",
    "for i, filepath in enumerate(pathnames):\n",
    "    \n",
    "    try:\n",
    "        with fits.open(filepath) as hdul:\n",
    "            if len(hdul) > 1 and hasattr(hdul[1], 'data'):\n",
    "                spec_data = hdul[1].data\n",
    "                loglam = spec_data['LOGLAM']\n",
    "                fl = spec_data['FLUX']\n",
    "                iv = spec_data['IVAR']\n",
    "                wa = 10**loglam\n",
    "                if wavelength is None:\n",
    "                    wavelength = wa\n",
    "                    M = len(wavelength)\n",
    "                    flux = np.ones((N, M))\n",
    "                    ivar = np.zeros_like(flux)\n",
    "                if np.allclose(wa, wavelength):\n",
    "                    flux[i] = fl / np.median(fl)\n",
    "                    ivar[i] = iv * np.median(fl) ** 2\n",
    "                else:\n",
    "                    print(f\"  Dropped {filepath}: bad wavelength grid\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Dropped {filepath}: {e}\")\n",
    "\n",
    "print(\"data blocks:\", flux.shape, ivar.shape, np.prod(flux.shape))\n",
    "print(\"bad pixels:\", np.sum(~ np.isfinite(flux)), np.sum(~ np.isfinite(ivar)),\n",
    "      np.sum(ivar <= 0.) / np.prod(flux.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ca7d3e-0d8f-4347-8ca2-8a39c6056421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trim data\n",
    "good = (wavelength > 3700) & (wavelength < 10300) # magic\n",
    "wavelength = wavelength[good]\n",
    "flux = flux[:, good]\n",
    "ivar = ivar[:, good]\n",
    "print(flux.shape, ivar.shape, wavelength.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ce1261-6f4e-4c2d-a17c-9a549b4ccd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# floor and ceil the ivars ## HACK\n",
    "maxivar = 1.e4 / flux ** 2 # magic -- nothing is known to better than 1 percent\n",
    "ivar = np.clip(ivar, 0., maxivar)\n",
    "maxivar = 1.e5 / np.median(flux, axis=1) ** 2 # magic -- nothing is known to better than 0.3 percent on average\n",
    "minivar = 1.e-5 / np.median(flux, axis=1) ** 2 # magic -- there is trivial information even at useless pixels\n",
    "ivar = np.clip(ivar, minivar[:, None], maxivar[:, None])\n",
    "print(np.min(ivar), np.max(ivar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da651bcd-9d85-4e36-b955-21e21af09cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make two disjoint subsets, A and B\n",
    "N, M = flux.shape\n",
    "rng = np.random.default_rng(17) # the most random number\n",
    "foo = np.random.uniform(size=N)\n",
    "A = foo < np.median(foo)\n",
    "B = np.logical_not(A)\n",
    "print(np.sum(A), np.sum(B), ~np.any(np.logical_and(A, B)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33901f65-2d36-4533-af94-c4d093b62e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# everything goes into two disjoint test sets\n",
    "Aidx = np.arange(N)[A]\n",
    "Bidx = np.arange(N)[B]\n",
    "print(len(Aidx), len(Bidx), np.all(A[Aidx]), np.all(B[Bidx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d08f864-5660-4e32-a8bc-9bf2781b554f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting utility: Hydrogen recombination lines\n",
    "\n",
    "def hydrogen_line(n_upper, n_lower):\n",
    "    R_H = 10973731.568157 # (12) per meter; Wikipedia\n",
    "    wave_number = R_H * (1/n_lower**2 - 1/n_upper**2) # per meter\n",
    "    return (1. / np.abs(wave_number)) * 1.e10 # Angstrom\n",
    "\n",
    "def plot_hydrogen_lines(ax):\n",
    "    # plt.axvline(6564.6, color=\"g\", lw=0.5, alpha=0.23) # true H-alpha rather than computed\n",
    "    for n1 in (2, 3):\n",
    "        for n2 in range(n1 + 1, n1 + 18): # magic\n",
    "            ax.axvline(hydrogen_line(n2, n1), color=\"b\", lw=0.5, alpha=0.23, zorder=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441df86a-05e2-423a-8821-368f529b4ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting utility: Hogg cares about wavelength axes.\n",
    "\n",
    "def hogg_wavelength_axis(ax, ws):\n",
    "    # plot_hydrogen_lines(ax)\n",
    "    for label, line in LINELIST:\n",
    "        if label[0:2] == \"He\":\n",
    "            plt.axvline(line, color=\"r\", lw=0.5, alpha=0.23, zorder=-1)\n",
    "    ax.semilogx()\n",
    "    ticks = [3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "    ticklabels = [str(tick) for tick in ticks]\n",
    "    ax.set_xticks(ticks, ticklabels)\n",
    "    ax.set_xlim(np.min(ws), np.max(ws))\n",
    "    # ax.set_xlim(6500, 6600) # zoom in\n",
    "    ax.set_xlabel('wavelength')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda24e3-3689-4fd4-a21b-e5158596283a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the eigenvectors of a model\n",
    "\n",
    "def plot_G(model, waves, title):\n",
    "    f = plt.figure(figsize=(12,8))\n",
    "    for k, g in enumerate(model.G):\n",
    "        plt.step(waves, 10. * g + k,\n",
    "                 where='mid', lw=0.5, alpha=0.90)\n",
    "    plt.ylim(-1., model.K)\n",
    "    ax = hogg_wavelength_axis(plt.gca(), waves)\n",
    "    plt.title(title)\n",
    "    fn = plot_folder + \"/\" + \"_\".join(title.split(\" \")) + \".png\"\n",
    "    print(f\"writing file {fn}\")\n",
    "    plt.savefig(fn)\n",
    "    plt.close(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea5da0b-8b60-47d4-ac27-82db13834362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a spectrum and a synthetic spectrum and residuals\n",
    "\n",
    "def plot_one_spectrum(waves, flux, ivar, name, prefix, synth=None, verbose=False, legend=None):\n",
    "    f = plt.figure(figsize=(12, 4))\n",
    "    plt.axhline(0., lw=0.5, color='k', alpha=0.45)\n",
    "    plt.step(waves, flux,\n",
    "             where='mid', color='k', lw=0.5, alpha=0.90)\n",
    "    tiny = 0.01 / np.median(flux) ** 2\n",
    "    flhi = flux + 1. / np.sqrt(ivar + tiny)\n",
    "    fllo = flux - 1. / np.sqrt(ivar + tiny)\n",
    "    plt.fill_between(waves, fllo, flhi,\n",
    "                     step='mid', color='k', alpha=0.23)\n",
    "    if synth is not None:\n",
    "        plt.step(waves, flux - synth,\n",
    "                 where='mid', color='k', lw=0.5, alpha=0.90)\n",
    "        plt.step(waves, synth,\n",
    "                 where='mid', color='r', lw=0.5, alpha=0.90)\n",
    "        plt.step(waves, np.zeros_like(flux),\n",
    "                 where='mid', color='r', lw=0.5, alpha=0.90)\n",
    "\n",
    "    # adjust axes\n",
    "    foo = np.nanpercentile(flux, 90)\n",
    "    plt.ylim(-0.15 * foo, 1.5 * foo)\n",
    "    plt.ylabel('flux')\n",
    "    plottitle = name\n",
    "    if legend is not None:\n",
    "        plottitle = plottitle + \" \" + legend\n",
    "    plt.title(plottitle)\n",
    "    hogg_wavelength_axis(plt.gca(), waves)\n",
    "\n",
    "    # Save plot\n",
    "    plot_filename = plot_folder + '/' + prefix + name + '.png'\n",
    "    plt.savefig(plot_filename)\n",
    "    plt.close(f)\n",
    "    if verbose:\n",
    "        print(f\"  Plot saved: {plot_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc9fb48-ddab-49ea-9010-4f833c21d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make test step but with a list of lines held out (like, say, H-alpha, H-beta, H-delta)\n",
    "\n",
    "def censored_cross_test(Y, W, waves, models, lines, delta):\n",
    "    W_line = 1. * W # copy\n",
    "    for line in lines:\n",
    "        near_line = (waves > (line - delta)) & (waves < (line + delta))\n",
    "        W_line[:, near_line] = 0.\n",
    "    return cross_test(Y, W_line, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f40285-387b-4cc5-8e2f-880bab46b17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross test: Test A objects with model B, and B objects with model A.\n",
    "\n",
    "def cross_test(Y, W, models, max_workers=16):\n",
    "    synth = np.zeros_like(Y) + np.nan\n",
    "    print(\"cross_test(): synthesizing with\", np.sum(np.isnan(synth)), \"pixels to go\")\n",
    "    for model, testidx in models.values():\n",
    "        G = 1. * model.G\n",
    "        Q2 = model.Q2\n",
    "        def test_one_index(i):\n",
    "            return i, rhmf.test(Y[i], W[i], G, Q2)\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = executor.map(test_one_index, testidx)\n",
    "            for i, s in results:\n",
    "                synth[i] = s\n",
    "        print(\"cross_test(): synthesizing with\", np.sum(np.isnan(synth)), \"pixels to go\")\n",
    "    return synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663454a-2461-41ab-bfbb-089e3db4478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# punk the rhmf model to do robust polynomial fitting\n",
    "# the requirement to do this shows that I am living my life wrong\n",
    "\n",
    "def robust_polyfit(ys, ws, xs, degree, nsigma=5.0):\n",
    "    design = np.vstack([xs ** d for d in range(degree + 1)])\n",
    "    poly = rhmf.RHMF(degree + 1, nsigma, G=design)\n",
    "    poly.M = len(xs)\n",
    "    poly.trained = True\n",
    "    return poly.test(ys, ws)\n",
    "\n",
    "def continuum_at_line(inys, inws, inxs, line):\n",
    "    near = np.abs(inxs - line) < 300 # magic\n",
    "    ys = np.append(inys[near], 0.) # hackitey hack\n",
    "    ws = np.append(inws[near], 0.)\n",
    "    xs = np.append(inxs[near], line)\n",
    "    verynear = np.abs(xs - line) < 10 # magic\n",
    "    ws[verynear] = 0.\n",
    "    return robust_polyfit(ys, ws, xs, 2)[-1]\n",
    "\n",
    "def integrate_line(dys, ws, xs, line):\n",
    "    \"\"\"\n",
    "    ## bugs:\n",
    "    - assumes terrible things about `xs`.\n",
    "    - has a terrible (but empirical) uncertainty analysis\n",
    "    \"\"\"\n",
    "    dxs = 0.5 * np.abs(xs - np.roll(xs, 1)) + 0.5 * np.abs(np.roll(xs, -1) - xs)\n",
    "    dxs[0], dxs[-1] = 0., 0.\n",
    "    close = np.abs(xs - line) < 400.0 # too far?\n",
    "    vvnear = np.abs(xs - line) < 4.0 # too close?\n",
    "    dyvar = 0.25 * np.percentile(dys[close] ** 2, 95) # woah cray\n",
    "    return np.sum(dys[vvnear] * dxs[vvnear]), np.sqrt(np.sum(dyvar * dxs[vvnear] * dxs[vvnear]))\n",
    "\n",
    "def line_ew(ys, ws, xs, ss, line):\n",
    "    flux, flerr = integrate_line(ys - ss, ws, xs, line)\n",
    "    cont = continuum_at_line(ys, ws, xs, line)\n",
    "    return flux / cont, flerr / cont\n",
    "\n",
    "def measure_all_line_ews(Y, W, waves, S, max_workers=100):\n",
    "    N = len(Y)\n",
    "    M = len(LINELIST)\n",
    "    ews = np.zeros((N, M)) + np.nan\n",
    "    ewerrs = np.zeros((N, M)) + np.nan\n",
    "    def measure_one_index(i):\n",
    "        if i % 10000 == 0:\n",
    "            print(\"measure_all_line_ews:\", i)\n",
    "        return i, [line_ew(Y[i], W[i], waves, S[i], line) for _, line in LINELIST]\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        results = executor.map(measure_one_index, range(N))\n",
    "        for i, foo in results:\n",
    "            ews[i] = [f[0] for f in foo]\n",
    "            ewerrs[i] = [f[1] for f in foo]\n",
    "    return ews, ewerrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d35616-6575-4a2c-958f-452ead237a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the full train and test pipeline\n",
    "\n",
    "def train_and_test(Y, W, waves, names, models, maxiter=10):\n",
    "    \n",
    "    # train step\n",
    "    for label in models.keys():\n",
    "        print(label)\n",
    "        model = models[label][0]\n",
    "        print(label, model.Y.shape)\n",
    "        model.train(maxiter=maxiter)\n",
    "        plot_G(model, waves, \"model \" + label)\n",
    "        plt.show()\n",
    "\n",
    "    # test step\n",
    "    censor_lines = [foo[1] for foo in LINELIST[0:2]] # H-alpha, H-beta\n",
    "    delta = 4. # Angstroms, magic\n",
    "    synth_ex_lines = censored_cross_test(Y, W, waves, models, censor_lines, delta)\n",
    "\n",
    "    # measure EWs\n",
    "    ews, ewerrs = measure_all_line_ews(flux, ivar, wavelength, synth_ex_lines)\n",
    "\n",
    "    # choose interesting emission objects to plot\n",
    "    p = -1 # He II\n",
    "    good = ((ews[:,p] / ewerrs[:,p]) > 5.) # magic 5\n",
    "    idx = (np.arange(len(ews))[good])\n",
    "    if len(idx) > 100:\n",
    "        idx = idx[:100]\n",
    "\n",
    "    # make plots\n",
    "    prefix = \"emitter_\"\n",
    "    pattern = f\"{plot_folder}/{prefix}*.png\"\n",
    "    os.system(f\"rm -f {pattern}\")\n",
    "    for i in idx:\n",
    "        thisprefix = prefix + f\"{ews[i, p]:4.2f}_\"\n",
    "        legend = f\"{LINELIST[p][0]} EW = {ews[i, p]:4.2f}+/-{ewerrs[i, p]:4.2f} Ang\"\n",
    "        plot_one_spectrum(waves, Y[i], W[i], names[i], thisprefix, synth=synth_ex_lines[i],\n",
    "                          legend=legend)\n",
    "    os.system(f\"./make_pdf.py {pattern} foo.pdf\")\n",
    "\n",
    "    # choose interesting absorption objects to plot\n",
    "    good = ((ews[:, p] / ewerrs[:, p]) < -5.)\n",
    "    idx = (np.arange(len(ews))[good])\n",
    "    if len(idx) > 100:\n",
    "        idx = idx[:100]\n",
    "\n",
    "    # make plots\n",
    "    prefix = \"absorber_\"\n",
    "    pattern = f\"{plot_folder}/{prefix}*.png\"\n",
    "    os.system(f\"rm -f {pattern}\")\n",
    "    for i in idx:\n",
    "        thisprefix = prefix + f\"{-ews[i, p]:4.2f}_\"\n",
    "        legend = f\"{LINELIST[p][0]} EW = -{-ews[i, p]:4.2f}+/-{ewerrs[i, p]:4.2f} Ang\"\n",
    "        plot_one_spectrum(waves, Y[i], W[i], names[i], thisprefix, synth=synth_ex_lines[i],\n",
    "                          legend=legend)\n",
    "    os.system(f\"./make_pdf.py {pattern} bar.pdf\")\n",
    "    \n",
    "    return synth_ex_lines, ews, ewerrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024997b9-eb08-49f2-a269-329754fe1c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_set(idx, considerations=None, target_size=1000):\n",
    "    \"\"\"\n",
    "    idx: list of indices\n",
    "    considerations: list of booleans\n",
    "    \"\"\"\n",
    "    if considerations is None:\n",
    "        considerations = np.ones_like(idx).astype(bool)\n",
    "    assert len(idx) == len(considerations)\n",
    "    factor = np.sum(considerations) // target_size\n",
    "    if factor < 1:\n",
    "        factor = 1\n",
    "    train = (idx[considerations])[::factor]\n",
    "    print(\"make_good_training_set(): chose\", len(train), \"from\", len(idx))\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5567b9-d1ea-4b1a-8ae7-d8051f35ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start models\n",
    "# format of each entry of `models` is `name: (model, test_indices)`\n",
    "models = {\"A\": (rhmf.RHMF(rank, nsigma), Bidx),  # model A used for B test set and vice versa\n",
    "          \"B\": (rhmf.RHMF(rank, nsigma), Aidx)}\n",
    "Atrain = make_training_set(Aidx, target_size=500)\n",
    "Btrain = make_training_set(Bidx, target_size=500)\n",
    "models[\"A\"][0].set_training_data(flux[Atrain], ivar[Atrain])\n",
    "models[\"B\"][0].set_training_data(flux[Btrain], ivar[Btrain])\n",
    "for key in models.keys():\n",
    "    print(key, models[key][0].Y.shape, models[key][0].input_W.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51526b4c-9e71-425a-8475-9643d553f8ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# start training\n",
    "synth, ews, ewerrs = train_and_test(flux, ivar, wavelength, specnames, models, maxiter=10)\n",
    "print(synth.shape, ews.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2121d8d1-027c-4f4b-9ae3-7d13213fc94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = 5\n",
    "for p in [0, 3, 4]:\n",
    "    good = (ewerrs[:, p] < 0.2) & (ewerrs[:, q] < 0.2) # magic numbers\n",
    "    plt.axvline(0., color=\"k\", lw=1, alpha=0.5)\n",
    "    plt.axhline(0., color=\"k\", lw=1, alpha=0.5)\n",
    "    plt.scatter(ews[good, p], ews[good, q], color=\"k\", s=2, alpha=0.45)\n",
    "    foo = np.percentile(ews[good, p], 99)\n",
    "    bar = np.percentile(ews[good, q], 99)\n",
    "    plt.xlim(-4 * foo, 4. * foo)\n",
    "    plt.ylim(-4 * bar, 4. * bar)\n",
    "    plt.xlabel(LINELIST[p][0] + \" EW (Ang)\")\n",
    "    plt.ylabel(LINELIST[q][0] + \" EW (Ang)\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce75d5d-1fc2-40fd-88a1-6d257b2a7594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train even more, but always removing stars with H-alpha emission-line issues\n",
    "# remove symmetrically because otherwise we may be doomed.\n",
    "for t in range(4):\n",
    "    print(\"iteration\", t + 1)\n",
    "    noHa = (np.abs(ews[:, 0] / ewerrs[:, 0]) < 2.)\n",
    "    print(\"fraction with no detectable narrow H-alpha deviation:\", np.sum(noHa) / len(ews))\n",
    "    Atrain = make_training_set(Aidx, considerations = noHa[Aidx], target_size=1000 + 200 * t)\n",
    "    Btrain = make_training_set(Bidx, considerations = noHa[Bidx], target_size=1000 + 200 * t)\n",
    "    models[\"A\"][0].set_training_data(flux[Atrain], ivar[Atrain])\n",
    "    models[\"B\"][0].set_training_data(flux[Btrain], ivar[Btrain])\n",
    "    synth, ews, ewerrs = train_and_test(flux, ivar, wavelength, specnames, models, maxiter=50 + 10 * t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf35e98-f297-40f4-91bf-f3c95cf2c0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth, ews, ewerrs = train_and_test(flux, ivar, wavelength, specnames, models, maxiter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c1ad0a-8955-4d5a-aed1-47dfd9baff30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
