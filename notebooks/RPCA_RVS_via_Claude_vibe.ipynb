{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/davidwhogg/RVSanomalies/blob/main/notebooks/RVS_meets_RPCA_via_Claude_vibe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Robust PCA Analysis of Gaia RVS Spectra\n",
        "This notebook implements Robust Principal Component Analysis (RPCA) on Gaia RVS spectra to decompose them into low-rank and sparse components, potentially revealing unusual spectral features, emission lines, or other anomalies.\n",
        "\n",
        "## Authors:\n",
        "- **David W. Hogg**\n",
        "- **Hans-Walter Rix**\n",
        "- **Claude**\n",
        "\n",
        "## Requirements:\n",
        "- pip install astroquery astropy numpy matplotlib scipy scikit-learn\n",
        "\n"
      ],
      "metadata": {
        "id": "hG3ooMBBljlY"
      },
      "id": "hG3ooMBBljlY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e08dd22d",
      "metadata": {
        "id": "e08dd22d"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from astroquery.gaia import Gaia\n",
        "from astropy.table import Table\n",
        "from astropy.io import fits\n",
        "import os\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "from scipy.linalg import svd\n",
        "from scipy.sparse.linalg import svds\n",
        "import pickle\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set up plotting parameters\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8511ed1c",
      "metadata": {
        "id": "8511ed1c"
      },
      "outputs": [],
      "source": [
        "## 1. Data Download Functions\n",
        "\n",
        "def find_rvs_sources_gspphot(teff_min=4810, teff_max=6200,\n",
        "                            logg_min=1.0, logg_max=3.0,\n",
        "                            grvs_mag_max=11.0, n_sources=500):\n",
        "    \"\"\"\n",
        "    Query Gaia archive for sources with RVS spectra using GSP-Phot parameters.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    teff_min, teff_max : float\n",
        "        Temperature range in K\n",
        "    logg_min, logg_max : float\n",
        "        Log gravity range\n",
        "    grvs_mag_max : float\n",
        "        Maximum GRVS magnitude\n",
        "    n_sources : int\n",
        "        Number of sources to retrieve\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    astropy.table.Table\n",
        "        Table of sources with RVS spectra\n",
        "    \"\"\"\n",
        "\n",
        "    query = f\"\"\"\n",
        "    SELECT TOP {n_sources}\n",
        "        source_id, ra, dec, phot_g_mean_mag, grvs_mag,\n",
        "        radial_velocity, radial_velocity_error,\n",
        "        teff_gspphot, logg_gspphot, mh_gspphot,\n",
        "        bp_rp, parallax\n",
        "    FROM gaiadr3.gaia_source\n",
        "    WHERE has_rvs = 't'\n",
        "    AND grvs_mag <= {grvs_mag_max}\n",
        "    AND teff_gspphot BETWEEN {teff_min} AND {teff_max}\n",
        "    AND logg_gspphot BETWEEN {logg_min} AND {logg_max}\n",
        "    AND teff_gspphot IS NOT NULL\n",
        "    AND logg_gspphot IS NOT NULL\n",
        "    AND radial_velocity IS NOT NULL\n",
        "    ORDER BY grvs_mag ASC\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"Querying Gaia archive for RVS sources...\")\n",
        "    print(f\"Criteria: {teff_min} <= Teff <= {teff_max} K\")\n",
        "    print(f\"         {logg_min} <= log g <= {logg_max}\")\n",
        "    print(f\"         GRVS mag <= {grvs_mag_max}\")\n",
        "    print(f\"         Requesting {n_sources} sources\")\n",
        "\n",
        "    job = Gaia.launch_job_async(query)\n",
        "    sources = job.get_results()\n",
        "\n",
        "    print(f\"\\nFound {len(sources)} sources matching criteria\")\n",
        "\n",
        "    return sources\n",
        "\n",
        "\n",
        "def download_rvs_spectrum(source_id, output_dir=\"rvs_spectra_cache\"):\n",
        "    \"\"\"\n",
        "    Download RVS spectrum for a single source.\n",
        "    Returns wavelength and flux arrays.\n",
        "    \"\"\"\n",
        "\n",
        "    # Check if spectrum already exists in cache\n",
        "    cache_file = os.path.join(output_dir, f\"rvs_{source_id}.npz\")\n",
        "    if os.path.exists(cache_file):\n",
        "        data = np.load(cache_file)\n",
        "        return data['wavelength'], data['flux']\n",
        "\n",
        "    try:\n",
        "        # Download spectrum\n",
        "        retrieval_type = 'RVS'\n",
        "        data_structure = 'INDIVIDUAL'\n",
        "        data_release = 'Gaia DR3'\n",
        "\n",
        "        datalink_products = Gaia.load_data(\n",
        "            ids=[str(source_id)],\n",
        "            data_release=data_release,\n",
        "            retrieval_type=retrieval_type,\n",
        "            data_structure=data_structure,\n",
        "            verbose=False\n",
        "        )\n",
        "\n",
        "        if not datalink_products:\n",
        "            return None, None\n",
        "\n",
        "        product_key = f\"RVS-Gaia DR3 {source_id}.xml\"\n",
        "\n",
        "        if product_key not in datalink_products:\n",
        "            return None, None\n",
        "\n",
        "        # Extract spectrum\n",
        "        votable = datalink_products[product_key][0]\n",
        "        spectrum_table = votable.to_table()\n",
        "\n",
        "        wavelength = np.array(spectrum_table['wavelength'])  # in nm\n",
        "        flux = np.array(spectrum_table['flux'])  # normalized\n",
        "\n",
        "        # Save to cache\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        np.savez(cache_file, wavelength=wavelength, flux=flux)\n",
        "\n",
        "        return wavelength, flux\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading spectrum for source {source_id}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "\n",
        "def download_multiple_spectra(sources, max_spectra=None):\n",
        "    \"\"\"\n",
        "    Download RVS spectra for multiple sources.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    spectra_data : dict\n",
        "        Dictionary with source_ids as keys and (wavelength, flux) tuples as values\n",
        "    \"\"\"\n",
        "\n",
        "    if max_spectra is None:\n",
        "        max_spectra = len(sources)\n",
        "\n",
        "    spectra_data = {}\n",
        "    successful_downloads = 0\n",
        "\n",
        "    print(f\"\\nDownloading RVS spectra for up to {max_spectra} sources...\")\n",
        "\n",
        "    # Check column names (Gaia returns uppercase)\n",
        "    if 'SOURCE_ID' in sources.colnames:\n",
        "        source_id_col = 'SOURCE_ID'\n",
        "    else:\n",
        "        source_id_col = 'source_id'\n",
        "\n",
        "    for i, source in enumerate(sources[:max_spectra]):\n",
        "        source_id = source[source_id_col]\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print(f\"Progress: {i}/{max_spectra} spectra processed...\")\n",
        "\n",
        "        wavelength, flux = download_rvs_spectrum(source_id)\n",
        "\n",
        "        if wavelength is not None and flux is not None:\n",
        "            spectra_data[source_id] = (wavelength, flux)\n",
        "            successful_downloads += 1\n",
        "\n",
        "    print(f\"\\nSuccessfully downloaded {successful_downloads} spectra\")\n",
        "\n",
        "    return spectra_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "712fac64",
      "metadata": {
        "id": "712fac64"
      },
      "outputs": [],
      "source": [
        "## 2. Data preprocessing\n",
        "\n",
        "def create_spectral_matrix(spectra_data, wavelength_grid=None, fill_value=1.0,\n",
        "                          n_clip_lower=8, n_clip_upper=3):\n",
        "    \"\"\"\n",
        "    Create a matrix Y where each row is a spectrum.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    spectra_data : dict\n",
        "        Dictionary of (wavelength, flux) tuples\n",
        "    wavelength_grid : array-like, optional\n",
        "        Common wavelength grid. If None, uses the first spectrum's grid\n",
        "    fill_value : float\n",
        "        Value to use for replacing NaN/Inf (default: 1.0 for continuum)\n",
        "    n_clip_lower : int\n",
        "        Number of pixels to clip from the lower wavelength end\n",
        "    n_clip_upper : int\n",
        "        Number of pixels to clip from the upper wavelength end\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Y : np.ndarray\n",
        "        Matrix of shape (n_spectra, n_wavelengths)\n",
        "    wavelength_grid : np.ndarray\n",
        "        Wavelength grid used\n",
        "    source_ids : list\n",
        "        List of source IDs in same order as rows of Y\n",
        "    bad_pixel_mask : np.ndarray\n",
        "        Boolean mask indicating replaced NaN/Inf values\n",
        "    \"\"\"\n",
        "\n",
        "    source_ids = list(spectra_data.keys())\n",
        "    n_spectra = len(source_ids)\n",
        "\n",
        "    # Use first spectrum to define wavelength grid if not provided\n",
        "    if wavelength_grid is None:\n",
        "        wavelength_grid = spectra_data[source_ids[0]][0]\n",
        "        # Apply clipping to wavelength grid\n",
        "        if n_clip_lower > 0 or n_clip_upper > 0:\n",
        "            wavelength_grid = wavelength_grid[n_clip_lower:len(wavelength_grid)-n_clip_upper]\n",
        "            print(f\"Clipping spectra: removing {n_clip_lower} pixels from lower end, \"\n",
        "                  f\"{n_clip_upper} pixels from upper end\")\n",
        "            print(f\"New wavelength range: {wavelength_grid[0]:.2f} - {wavelength_grid[-1]:.2f} nm\")\n",
        "\n",
        "    n_wavelengths = len(wavelength_grid)\n",
        "\n",
        "    # Initialize spectral matrix and bad pixel mask\n",
        "    Y = np.zeros((n_spectra, n_wavelengths))\n",
        "    bad_pixel_mask = np.zeros((n_spectra, n_wavelengths), dtype=bool)\n",
        "\n",
        "    # Track statistics\n",
        "    total_bad_pixels = 0\n",
        "    spectra_with_bad_pixels = 0\n",
        "\n",
        "    # Fill matrix\n",
        "    for i, source_id in enumerate(source_ids):\n",
        "        wavelength, flux = spectra_data[source_id]\n",
        "\n",
        "        # Apply clipping to flux\n",
        "        if n_clip_lower > 0 or n_clip_upper > 0:\n",
        "            flux = flux[n_clip_lower:len(flux)-n_clip_upper]\n",
        "\n",
        "        # Handle different length spectra\n",
        "        if len(flux) != n_wavelengths:\n",
        "            print(f\"Warning: Spectrum {source_id} has different length ({len(flux)} vs {n_wavelengths})\")\n",
        "            # Simple truncation or padding\n",
        "            min_len = min(len(flux), n_wavelengths)\n",
        "            flux_adjusted = np.full(n_wavelengths, fill_value)\n",
        "            flux_adjusted[:min_len] = flux[:min_len]\n",
        "            flux = flux_adjusted\n",
        "\n",
        "        # Find bad pixels (NaN or Inf)\n",
        "        bad_pixels = np.isnan(flux) | np.isinf(flux)\n",
        "        if np.any(bad_pixels):\n",
        "            spectra_with_bad_pixels += 1\n",
        "            total_bad_pixels += np.sum(bad_pixels)\n",
        "            bad_pixel_mask[i, :] = bad_pixels\n",
        "\n",
        "            # Replace bad pixels with fill_value\n",
        "            flux = np.where(bad_pixels, fill_value, flux)\n",
        "\n",
        "        Y[i, :] = flux\n",
        "\n",
        "    print(f\"\\nBad pixel statistics:\")\n",
        "    print(f\"  Spectra with bad pixels: {spectra_with_bad_pixels}/{n_spectra}\")\n",
        "    print(f\"  Total bad pixels: {total_bad_pixels}\")\n",
        "    print(f\"  Bad pixels replaced with: {fill_value}\")\n",
        "\n",
        "    return Y, wavelength_grid, source_ids, bad_pixel_mask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5672bf1b",
      "metadata": {
        "id": "5672bf1b"
      },
      "outputs": [],
      "source": [
        "## 3. Robust PCA Implementation\n",
        "\n",
        "class RobustPCA:\n",
        "    \"\"\"\n",
        "    Robust PCA using Alternating Direction Method of Multipliers (ADMM)\n",
        "\n",
        "    Decomposes a matrix Y = L + S where:\n",
        "    - L is low-rank\n",
        "    - S is sparse\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, lambda_param=None, mu=None, tol=1e-6, max_iter=400):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "        -----------\n",
        "        lambda_param : float\n",
        "            Regularization parameter for sparse component\n",
        "            Default: 1/sqrt(max(m,n))\n",
        "        mu : float\n",
        "            ADMM penalty parameter\n",
        "            Default: 0.25/mean(abs(Y))\n",
        "        tol : float\n",
        "            Convergence tolerance\n",
        "        max_iter : int\n",
        "            Maximum iterations\n",
        "        \"\"\"\n",
        "        self.lambda_param = lambda_param\n",
        "        self.mu = mu\n",
        "        self.tol = tol\n",
        "        self.max_iter = max_iter\n",
        "        self.L = None\n",
        "        self.S = None\n",
        "\n",
        "    def fit(self, Y):\n",
        "        \"\"\"\n",
        "        Fit Robust PCA model to data matrix Y.\n",
        "        \"\"\"\n",
        "        m, n = Y.shape\n",
        "\n",
        "        # Set default parameters if not provided\n",
        "        if self.lambda_param is None:\n",
        "            self.lambda_param = 1. / np.sqrt(max(m, n))\n",
        "\n",
        "        if self.mu is None:\n",
        "            self.mu = 0.25 / np.mean(np.abs(Y))\n",
        "\n",
        "        # Initialize variables\n",
        "        L = np.zeros_like(Y)\n",
        "        S = np.zeros_like(Y)\n",
        "        Z = np.zeros_like(Y)\n",
        "\n",
        "        # ADMM iterations\n",
        "        print(f\"Starting Robust PCA with lambda={self.lambda_param:.4f}, mu={self.mu:.4f}\")\n",
        "\n",
        "        for iter_num in range(self.max_iter):\n",
        "            # Update L (low-rank component) via SVD shrinkage\n",
        "            U, sigma, Vt = svd(Y - S + Z/self.mu, full_matrices=False)\n",
        "            sigma_shrink = np.maximum(sigma - 1/self.mu, 0)\n",
        "            L = U @ np.diag(sigma_shrink) @ Vt\n",
        "\n",
        "            # Update S (sparse component) via soft thresholding\n",
        "            S = self._soft_threshold(Y - L + Z/self.mu, self.lambda_param/self.mu)\n",
        "            noise_threshold = 3.0 * np.std(Y - L, axis=0)  # Simple noise estimate\n",
        "            S = S * (np.abs(S) > noise_threshold[np.newaxis, :])  # Zero out below noise\n",
        "\n",
        "            # Update dual variable Z\n",
        "            Z = Z + self.mu * (Y - L - S)\n",
        "\n",
        "            # Check convergence\n",
        "            primal_residual = np.linalg.norm(Y - L - S, 'fro')\n",
        "\n",
        "            if iter_num % 50 == 0:\n",
        "                rank_L = np.sum(sigma_shrink > 0)\n",
        "                sparsity_S = np.sum(np.abs(S) > 1e-6) / S.size\n",
        "                print(f\"Iter {iter_num}: residual={primal_residual:.6f}, \"\n",
        "                      f\"rank(L)={rank_L}, sparsity(S)={sparsity_S:.3%}\")\n",
        "\n",
        "            if primal_residual < self.tol:\n",
        "                print(f\"Converged after {iter_num + 1} iterations\")\n",
        "                break\n",
        "\n",
        "        self.L = L\n",
        "        self.S = S\n",
        "\n",
        "        # Compute final statistics\n",
        "        self.rank = np.linalg.matrix_rank(L)\n",
        "        self.sparsity = np.sum(np.abs(S) > 1e-6) / S.size\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _soft_threshold(self, X, threshold):\n",
        "        \"\"\"Soft thresholding operator.\"\"\"\n",
        "        return np.sign(X) * np.maximum(np.abs(X) - threshold, 0)\n",
        "\n",
        "    def transform(self, Y):\n",
        "        \"\"\"Transform new data using fitted model.\"\"\"\n",
        "        if self.L is None:\n",
        "            raise ValueError(\"Model not fitted yet\")\n",
        "        # For new data, would need to project onto learned subspace\n",
        "        # For now, just return decomposition of training data\n",
        "        return self.L, self.S"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 4. Analysis Functions\n",
        "\n",
        "def analyze_low_rank_component(L, wavelength_grid, n_components=5):\n",
        "    \"\"\"\n",
        "    Analyze the low-rank component using SVD.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    components : dict\n",
        "        Dictionary with 'U', 'S', 'V' from SVD\n",
        "    \"\"\"\n",
        "\n",
        "    U, S, Vt = svd(L, full_matrices=False)\n",
        "\n",
        "    components = {\n",
        "        'U': U[:, :n_components],  # Spectral coefficients\n",
        "        'S': S[:n_components],     # Singular values\n",
        "        'V': Vt[:n_components, :], # Principal spectral components\n",
        "        'wavelength': wavelength_grid\n",
        "    }\n",
        "\n",
        "    return components\n",
        "\n",
        "\n",
        "def find_sparse_outliers(S, source_ids, threshold_percentile=95):\n",
        "    \"\"\"\n",
        "    Find spectra with significant sparse contributions.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    outlier_info : dict\n",
        "        Information about outlier spectra\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute sparse contribution per spectrum\n",
        "    sparse_norms = np.linalg.norm(S, axis=1)\n",
        "    threshold = np.percentile(sparse_norms, threshold_percentile)\n",
        "\n",
        "    outlier_mask = sparse_norms > threshold\n",
        "    outlier_indices = np.where(outlier_mask)[0]\n",
        "\n",
        "    outlier_info = {\n",
        "        'indices': outlier_indices,\n",
        "        'source_ids': [source_ids[i] for i in outlier_indices],\n",
        "        'sparse_norms': sparse_norms[outlier_indices],\n",
        "        'threshold': threshold,\n",
        "        'all_norms': sparse_norms\n",
        "    }\n",
        "\n",
        "    return outlier_info\n",
        "\n",
        "\n",
        "def identify_sparse_features(S_spectrum, wavelength, feature_threshold=0.05):\n",
        "    \"\"\"\n",
        "    Identify significant features in a sparse spectrum.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    S_spectrum : array\n",
        "        Sparse component for one spectrum\n",
        "    wavelength : array\n",
        "        Wavelength grid\n",
        "    feature_threshold : float\n",
        "        Threshold for significant features (relative to max)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    features : dict\n",
        "        Dictionary of identified features\n",
        "    \"\"\"\n",
        "\n",
        "    # Find peaks in absolute sparse component\n",
        "    abs_S = np.abs(S_spectrum)\n",
        "    max_S = np.max(abs_S)\n",
        "\n",
        "    if max_S < 1e-6:\n",
        "        return {'emission': [], 'absorption': [], 'wavelengths': []}\n",
        "\n",
        "    # Find significant features\n",
        "    significant_mask = abs_S > feature_threshold * max_S\n",
        "    feature_indices = np.where(significant_mask)[0]\n",
        "\n",
        "    emission_features = []\n",
        "    absorption_features = []\n",
        "\n",
        "    for idx in feature_indices:\n",
        "        if S_spectrum[idx] > 0:\n",
        "            emission_features.append((wavelength[idx], S_spectrum[idx]))\n",
        "        else:\n",
        "            absorption_features.append((wavelength[idx], S_spectrum[idx]))\n",
        "\n",
        "    features = {\n",
        "        'emission': emission_features,\n",
        "        'absorption': absorption_features,\n",
        "        'wavelengths': wavelength[feature_indices],\n",
        "        'values': S_spectrum[feature_indices]\n",
        "    }\n",
        "\n",
        "    return features"
      ],
      "metadata": {
        "id": "E82jqxhHmZ3D"
      },
      "id": "E82jqxhHmZ3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541a13a2",
      "metadata": {
        "id": "541a13a2"
      },
      "outputs": [],
      "source": [
        "## 5. Visualization Functions\n",
        "\n",
        "def plot_rpca_components(rpca_model, wavelength_grid, n_examples=3):\n",
        "    \"\"\"\n",
        "    Visualize the Robust PCA decomposition.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "    # Plot 1: Singular values\n",
        "    U, S, Vt = svd(rpca_model.L, full_matrices=False)\n",
        "    axes[0, 0].semilogy(S[:50], 'bo-')\n",
        "    axes[0, 0].set_xlabel('Component')\n",
        "    axes[0, 0].set_ylabel('Singular Value')\n",
        "    axes[0, 0].set_title('Singular Values of Low-Rank Component')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 2: First few principal components\n",
        "    ax = axes[0, 1]\n",
        "    for i in range(min(5, len(S))):\n",
        "        ax.plot(wavelength_grid, Vt[i, :] + i*0.5,\n",
        "                label=f'PC{i+1} (σ={S[i]:.2f})')\n",
        "    ax.set_xlabel('Wavelength (nm)')\n",
        "    ax.set_ylabel('Principal Component (offset)')\n",
        "    ax.set_title('First 5 Principal Spectral Components')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 3: Distribution of sparse values\n",
        "    ax = axes[1, 0]\n",
        "    sparse_values = rpca_model.S.flatten()\n",
        "    sparse_nonzero = sparse_values[np.abs(sparse_values) > 1e-6]\n",
        "    ax.hist(sparse_nonzero, bins=50, alpha=0.7, color='red')\n",
        "    ax.set_xlabel('Sparse Component Value')\n",
        "    ax.set_ylabel('Count')\n",
        "    ax.set_title(f'Distribution of Non-zero Sparse Values\\n'\n",
        "                 f'Sparsity: {rpca_model.sparsity:.1%}')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot 4: Example sparse spectrum\n",
        "    ax = axes[1, 1]\n",
        "    sparse_norms = np.linalg.norm(rpca_model.S, axis=1)\n",
        "    top_sparse_idx = np.argsort(sparse_norms)[-n_examples:]\n",
        "\n",
        "    for i, idx in enumerate(top_sparse_idx):\n",
        "        ax.plot(wavelength_grid, rpca_model.S[idx, :] + i*0.1,\n",
        "                label=f'Spectrum {idx}')\n",
        "\n",
        "    ax.set_xlabel('Wavelength (nm)')\n",
        "    ax.set_ylabel('Sparse Component (offset)')\n",
        "    ax.set_title('Top Sparse Spectra')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Mark Ca II triplet positions (vacuum wavelengths)\n",
        "    ca_lines_vacuum = [849.8023, 854.4444, 866.4536]\n",
        "    for ca_line in ca_lines_vacuum:\n",
        "        ax.axvline(ca_line, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_spectrum_decomposition(Y, L, S, idx, wavelength_grid, source_id=None, return_fig=False):\n",
        "    \"\"\"\n",
        "    Plot the decomposition of a single spectrum.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, axes = plt.subplots(4, 1, figsize=(12, 10), sharex=True)\n",
        "\n",
        "    title_suffix = f\" (Source {source_id})\" if source_id else f\" (Index {idx})\"\n",
        "\n",
        "    # Original spectrum\n",
        "    axes[0].plot(wavelength_grid, Y[idx, :], 'b-', linewidth=1.5)\n",
        "    axes[0].set_ylabel('Flux')\n",
        "    axes[0].set_title(f'Original Spectrum{title_suffix}')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].set_ylim(0, 1.2)\n",
        "\n",
        "    # Low-rank component\n",
        "    axes[1].plot(wavelength_grid, L[idx, :], 'g-', linewidth=1.5)\n",
        "    axes[1].set_ylabel('Flux')\n",
        "    axes[1].set_title('Low-Rank Component')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].set_ylim(0, 1.2)\n",
        "\n",
        "    # Sparse component\n",
        "    axes[2].plot(wavelength_grid, S[idx, :], 'r-', linewidth=1.5)\n",
        "    axes[2].set_ylabel('Flux')\n",
        "    axes[2].set_title(f'Sparse Component (||S||₂ = {np.linalg.norm(S[idx, :]):.4f})')\n",
        "    axes[2].grid(True, alpha=0.3)\n",
        "\n",
        "    # Residual\n",
        "    residual = Y[idx, :] - L[idx, :] - S[idx, :]\n",
        "    axes[3].plot(wavelength_grid, residual, 'k-', linewidth=1.5)\n",
        "    axes[3].set_ylabel('Flux')\n",
        "    axes[3].set_title(f'Residual (||r||₂ = {np.linalg.norm(residual):.4f})')\n",
        "    axes[3].set_xlabel('Wavelength (nm)')\n",
        "    axes[3].grid(True, alpha=0.3)\n",
        "\n",
        "    # Mark Ca II triplet (vacuum wavelengths)\n",
        "    ca_lines_vacuum = [849.8023, 854.4444, 866.4536]\n",
        "    for ax in axes:\n",
        "        for ca_line in ca_lines_vacuum:\n",
        "            ax.axvline(ca_line, color='gray', linestyle='--', alpha=0.5)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if return_fig:\n",
        "        return fig\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "def plot_outlier_analysis(Y, S, outlier_info, wavelength_grid, source_ids,\n",
        "                         sources_table, n_examples=5):\n",
        "    \"\"\"\n",
        "    Detailed analysis of outlier spectra.\n",
        "    \"\"\"\n",
        "\n",
        "    n_outliers = min(n_examples, len(outlier_info['indices']))\n",
        "\n",
        "    if n_outliers == 0:\n",
        "        print(\"No outliers found!\")\n",
        "        return\n",
        "\n",
        "    fig, axes = plt.subplots(n_outliers, 2, figsize=(15, 4*n_outliers))\n",
        "\n",
        "    if n_outliers == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    # Check column names\n",
        "    source_id_col = 'SOURCE_ID' if 'SOURCE_ID' in sources_table.colnames else 'source_id'\n",
        "    teff_col = 'TEFF_GSPPHOT' if 'TEFF_GSPPHOT' in sources_table.colnames else 'teff_gspphot'\n",
        "    logg_col = 'LOGG_GSPPHOT' if 'LOGG_GSPPHOT' in sources_table.colnames else 'logg_gspphot'\n",
        "    grvs_col = 'GRVS_MAG' if 'GRVS_MAG' in sources_table.colnames else 'grvs_mag'\n",
        "\n",
        "    for i in range(n_outliers):\n",
        "        idx = outlier_info['indices'][i]\n",
        "        source_id = source_ids[idx]\n",
        "\n",
        "        # Find source info\n",
        "        source_mask = sources_table[source_id_col] == source_id\n",
        "        if np.any(source_mask):\n",
        "            source_info = sources_table[source_mask][0]\n",
        "            info_str = (f\"Teff={source_info[teff_col]:.0f}K, \"\n",
        "                       f\"log g={source_info[logg_col]:.2f}, \"\n",
        "                       f\"GRVS={source_info[grvs_col]:.2f}\")\n",
        "        else:\n",
        "            info_str = \"No info\"\n",
        "\n",
        "        # Plot original spectrum\n",
        "        ax = axes[i, 0]\n",
        "        ax.plot(wavelength_grid, Y[idx, :], 'b-', alpha=0.7, label='Original')\n",
        "        ax.plot(wavelength_grid, Y[idx, :] - S[idx, :], 'g-', alpha=0.7,\n",
        "                label='Low-rank approx')\n",
        "        ax.set_ylabel('Flux')\n",
        "        ax.set_title(f'Source {source_id}\\n{info_str}')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(0, 1.2)\n",
        "\n",
        "        # Plot sparse component\n",
        "        ax = axes[i, 1]\n",
        "        ax.plot(wavelength_grid, S[idx, :], 'r-', linewidth=1.5)\n",
        "        ax.axhline(0, color='k', linestyle='-', alpha=0.3)\n",
        "        ax.set_ylabel('Sparse Component')\n",
        "        ax.set_title(f'Sparse Component (||S||₂ = {outlier_info[\"sparse_norms\"][i]:.4f})')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Identify and mark features\n",
        "        features = identify_sparse_features(S[idx, :], wavelength_grid)\n",
        "\n",
        "        # Mark emission features\n",
        "        for wave, value in features['emission']:\n",
        "            ax.plot(wave, value, 'ro', markersize=8)\n",
        "            ax.annotate(f'{wave:.1f}', (wave, value),\n",
        "                       xytext=(5, 5), textcoords='offset points',\n",
        "                       fontsize=8, color='red')\n",
        "\n",
        "        # Mark absorption features\n",
        "        for wave, value in features['absorption']:\n",
        "            ax.plot(wave, value, 'bo', markersize=8)\n",
        "            ax.annotate(f'{wave:.1f}', (wave, value),\n",
        "                       xytext=(5, -10), textcoords='offset points',\n",
        "                       fontsize=8, color='blue')\n",
        "\n",
        "        if i == n_outliers - 1:\n",
        "            ax.set_xlabel('Wavelength (nm)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd077ea",
      "metadata": {
        "id": "bfd077ea"
      },
      "outputs": [],
      "source": [
        "## 6. Main Analysis Pipeline\n",
        "\n",
        "def main_analysis():\n",
        "    \"\"\"\n",
        "    Main analysis pipeline.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set parameters\n",
        "    params = {\n",
        "        'teff_min': 4200,\n",
        "        'teff_max': 4800,\n",
        "        'logg_min': 1.0,\n",
        "        'logg_max': 3.0,\n",
        "        'grvs_mag_max': 11.0,\n",
        "        'n_sources': 2000\n",
        "    }\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Robust PCA Analysis of Gaia RVS Spectra\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"\\nParameters:\")\n",
        "    for key, value in params.items():\n",
        "        print(f\"  {key}: {value}\")\n",
        "\n",
        "    # Step 1: Query Gaia archive\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 1: Querying Gaia Archive\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    sources = find_rvs_sources_gspphot(**params)\n",
        "\n",
        "    # Save source table\n",
        "    sources.write('gaia_rvs_sources.fits', format='fits', overwrite=True)\n",
        "    print(f\"Source table saved to 'gaia_rvs_sources.fits'\")\n",
        "\n",
        "    # Step 2: Download spectra\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 2: Downloading RVS Spectra\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    spectra_data = download_multiple_spectra(sources, max_spectra=params['n_sources'])\n",
        "\n",
        "    # Save spectra data\n",
        "    with open('spectra_data.pkl', 'wb') as f:\n",
        "        pickle.dump(spectra_data, f)\n",
        "    print(f\"Spectra data saved to 'spectra_data.pkl'\")\n",
        "\n",
        "    # Step 3: Create spectral matrix\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 3: Creating Spectral Matrix\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    #Y, wavelength_grid, source_ids, bad_pixel_mask = create_spectral_matrix(spectra_data)\n",
        "    Y, wavelength_grid, source_ids, bad_pixel_mask = create_spectral_matrix(spectra_data, n_clip_lower=8, n_clip_upper=3)\n",
        "    print(f\"Spectral matrix shape: {Y.shape}\")\n",
        "    print(f\"Wavelength range: {wavelength_grid[0]:.2f} - {wavelength_grid[-1]:.2f} nm\")\n",
        "\n",
        "    # Additional diagnostics\n",
        "    print(f\"\\nSpectral matrix statistics:\")\n",
        "    print(f\"  Min flux: {np.min(Y):.4f}\")\n",
        "    print(f\"  Max flux: {np.max(Y):.4f}\")\n",
        "    print(f\"  Mean flux: {np.mean(Y):.4f}\")\n",
        "    print(f\"  Std flux: {np.std(Y):.4f}\")\n",
        "    print(f\"  Contains NaN: {np.any(np.isnan(Y))}\")\n",
        "    print(f\"  Contains Inf: {np.any(np.isinf(Y))}\")\n",
        "\n",
        "    # Check if matrix is valid\n",
        "    if Y.shape[0] == 0:\n",
        "        print(\"ERROR: No valid spectra found in the data!\")\n",
        "        return None\n",
        "\n",
        "    # Step 4: Apply Robust PCA\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 4: Applying Robust PCA\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    rpca = RobustPCA()\n",
        "    rpca.fit(Y)\n",
        "\n",
        "    print(f\"\\nRobust PCA Results:\")\n",
        "    print(f\"  Rank of low-rank component: {rpca.rank}\")\n",
        "    print(f\"  Sparsity of sparse component: {rpca.sparsity:.2%}\")\n",
        "    print(f\"  Reconstruction error: {np.linalg.norm(Y - rpca.L - rpca.S, 'fro'):.6f}\")\n",
        "\n",
        "    # Save RPCA results\n",
        "    np.savez('rpca_results.npz',\n",
        "             L=rpca.L, S=rpca.S, Y=Y,\n",
        "             wavelength=wavelength_grid,\n",
        "             source_ids=source_ids,\n",
        "             bad_pixel_mask=bad_pixel_mask)\n",
        "    print(f\"RPCA results saved to 'rpca_results.npz'\")\n",
        "\n",
        "    # Step 5: Analyze components\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 5: Analyzing Components\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Analyze bad pixels in sparse component\n",
        "    print(\"\\nAnalyzing bad pixels in sparse component:\")\n",
        "\n",
        "    # Find which bad pixels ended up in sparse component\n",
        "    sparse_mask = np.abs(rpca.S) > 1e-6\n",
        "    bad_in_sparse = bad_pixel_mask & sparse_mask\n",
        "\n",
        "    n_bad_pixels = np.sum(bad_pixel_mask)\n",
        "    n_bad_in_sparse = np.sum(bad_in_sparse)\n",
        "\n",
        "    print(f\"  Total bad pixels (NaN/Inf): {n_bad_pixels}\")\n",
        "    print(f\"  Bad pixels identified in sparse component: {n_bad_in_sparse}\")\n",
        "    if n_bad_pixels > 0:\n",
        "        print(f\"  Percentage of bad pixels caught by sparse component: {100*n_bad_in_sparse/n_bad_pixels:.1f}%\")\n",
        "\n",
        "    # Analyze low-rank component\n",
        "    components = analyze_low_rank_component(rpca.L, wavelength_grid)\n",
        "    print(f\"\\nPrincipal component analysis:\")\n",
        "    print(f\"  Explained variance ratios: {components['S'][:5]**2 / np.sum(components['S']**2)}\")\n",
        "\n",
        "    # Find outliers\n",
        "    outlier_info = find_sparse_outliers(rpca.S, source_ids, threshold_percentile=90)\n",
        "    print(f\"\\nOutlier analysis:\")\n",
        "    print(f\"  Number of outliers (>90th percentile): {len(outlier_info['indices'])}\")\n",
        "    print(f\"  Outlier source IDs: {outlier_info['source_ids'][:10]}\")\n",
        "\n",
        "    # Step 6: Visualizations\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 6: Creating Visualizations\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Plot RPCA components overview\n",
        "    plot_rpca_components(rpca, wavelength_grid)\n",
        "\n",
        "    # Create directory for individual spectrum plots\n",
        "    plot_dir = \"rpca_spectrum_plots\"\n",
        "    os.makedirs(plot_dir, exist_ok=True)\n",
        "    print(f\"\\nCreated directory '{plot_dir}' for individual spectrum plots\")\n",
        "\n",
        "    # Sort ALL spectra by sparse component norm and plot top 20\n",
        "    print(\"\\nSorting spectra by sparse component norm...\")\n",
        "    sparse_norms = np.linalg.norm(rpca.S, axis=1)\n",
        "    sorted_indices = np.argsort(sparse_norms)[::-1]  # Descending order\n",
        "\n",
        "    n_top = 20\n",
        "    print(f\"\\nPlotting top {n_top} spectra with largest sparse components:\")\n",
        "\n",
        "    for rank, idx in enumerate(sorted_indices[:n_top]):\n",
        "        source_id = source_ids[idx]\n",
        "        sparse_norm = sparse_norms[idx]\n",
        "        print(f\"  Rank {rank+1}: Source {source_id}, ||S||₂ = {sparse_norm:.4f}\")\n",
        "\n",
        "        # Create the 4-panel plot\n",
        "        fig = plot_spectrum_decomposition(Y, rpca.L, rpca.S, idx, wavelength_grid,\n",
        "                                        source_id=source_id, return_fig=True)\n",
        "\n",
        "        # Save to file\n",
        "        filename = f\"spectrum_rank{rank+1:02d}_source{source_id}_sparsenorm{sparse_norm:.4f}.png\"\n",
        "        filepath = os.path.join(plot_dir, filename)\n",
        "        fig.savefig(filepath, dpi=150, bbox_inches='tight')\n",
        "        plt.close(fig)  # Close to save memory\n",
        "\n",
        "        print(f\"    Saved: {filename}\")\n",
        "\n",
        "    print(f\"\\nAll {n_top} spectrum plots saved to '{plot_dir}/' directory\")\n",
        "\n",
        "    # Create scatter plot of bad pixels vs sparse pixels\n",
        "    print(\"\\nCreating bad pixels vs sparse pixels scatter plot...\")\n",
        "\n",
        "    # Count bad pixels and sparse pixels per spectrum\n",
        "    n_bad_per_spectrum = np.sum(bad_pixel_mask, axis=1)\n",
        "    sparse_mask = np.abs(rpca.S) > 1e-6\n",
        "    n_sparse_per_spectrum = np.sum(sparse_mask, axis=1)\n",
        "\n",
        "    # Create scatter plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "\n",
        "    # Color points by sparse norm\n",
        "    scatter = ax.scatter(n_bad_per_spectrum, n_sparse_per_spectrum,\n",
        "                        c=sparse_norms, cmap='viridis', alpha=0.6, s=50)\n",
        "\n",
        "    # Add colorbar\n",
        "    cbar = plt.colorbar(scatter, ax=ax)\n",
        "    cbar.set_label('||S||₂ (Sparse Norm)', fontsize=12)\n",
        "\n",
        "    # Add diagonal line for reference\n",
        "    max_val = max(np.max(n_bad_per_spectrum), np.max(n_sparse_per_spectrum))\n",
        "    if max_val > 0:\n",
        "        ax.plot([0, max_val], [0, max_val], 'k--', alpha=0.3, label='1:1 line')\n",
        "\n",
        "    ax.set_xlabel('Number of Bad Pixels (NaN/Inf)', fontsize=12)\n",
        "    ax.set_ylabel('Number of Sparse Pixels (|S| > 1e-6)', fontsize=12)\n",
        "    ax.set_title('Bad Pixels vs Sparse Pixels for All Spectra', fontsize=14)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Add statistics to plot\n",
        "    correlation = np.corrcoef(n_bad_per_spectrum, n_sparse_per_spectrum)[0, 1]\n",
        "    textstr = f'Total spectra: {len(n_bad_per_spectrum)}\\n'\n",
        "    textstr += f'Spectra with bad pixels: {np.sum(n_bad_per_spectrum > 0)}\\n'\n",
        "    textstr += f'Correlation: {correlation:.3f}'\n",
        "    ax.text(0.05, 0.95, textstr, transform=ax.transAxes, fontsize=10,\n",
        "            verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save scatter plot\n",
        "    scatter_filename = 'bad_vs_sparse_pixels_scatter.png'\n",
        "    plt.savefig(scatter_filename, dpi=150, bbox_inches='tight')\n",
        "    print(f\"Scatter plot saved to '{scatter_filename}'\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Still do outlier analysis but with fewer examples\n",
        "    print(\"\\nPlotting outlier analysis (top 5)...\")\n",
        "    outlier_info['indices'] = sorted_indices[:5]  # Use top 5 for the outlier plot\n",
        "    outlier_info['source_ids'] = [source_ids[i] for i in sorted_indices[:5]]\n",
        "    outlier_info['sparse_norms'] = sparse_norms[sorted_indices[:5]]\n",
        "\n",
        "    plot_outlier_analysis(Y, rpca.S, outlier_info, wavelength_grid,\n",
        "                         source_ids, sources, n_examples=5)\n",
        "\n",
        "    # Step 7: Feature statistics\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"STEP 7: Feature Statistics\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    all_emission = []\n",
        "    all_absorption = []\n",
        "\n",
        "    for idx in outlier_info['indices']:\n",
        "        features = identify_sparse_features(rpca.S[idx, :], wavelength_grid)\n",
        "        all_emission.extend([w for w, _ in features['emission']])\n",
        "        all_absorption.extend([w for w, _ in features['absorption']])\n",
        "\n",
        "    if all_emission:\n",
        "        print(f\"\\nEmission features found at wavelengths:\")\n",
        "        unique_emission = np.unique(np.round(all_emission, 1))\n",
        "        emission_counts = {wave: all_emission.count(wave) for wave in unique_emission}\n",
        "        for wave, count in sorted(emission_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            print(f\"  {wave:.1f} nm: {count} occurrences\")\n",
        "\n",
        "    if all_absorption:\n",
        "        print(f\"\\nAbsorption features found at wavelengths:\")\n",
        "        unique_absorption = np.unique(np.round(all_absorption, 1))\n",
        "        absorption_counts = {wave: all_absorption.count(wave) for wave in unique_absorption}\n",
        "        for wave, count in sorted(absorption_counts.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
        "            print(f\"  {wave:.1f} nm: {count} occurrences\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Analysis complete!\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    return {\n",
        "        'sources': sources,\n",
        "        'spectra_data': spectra_data,\n",
        "        'Y': Y,\n",
        "        'wavelength_grid': wavelength_grid,\n",
        "        'source_ids': source_ids,\n",
        "        'rpca': rpca,\n",
        "        'components': components,\n",
        "        'outlier_info': outlier_info,\n",
        "        'bad_pixel_mask': bad_pixel_mask\n",
        "    }\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1560be",
      "metadata": {
        "id": "ac1560be"
      },
      "outputs": [],
      "source": [
        "## 7. Additional Analysis Functions (Optional)\n",
        "\n",
        "def analyze_spectral_type_differences(results):\n",
        "    \"\"\"\n",
        "    Analyze how RPCA components differ by stellar parameters.\n",
        "    \"\"\"\n",
        "\n",
        "    sources = results['sources']\n",
        "    rpca = results['rpca']\n",
        "    source_ids = results['source_ids']\n",
        "\n",
        "    # Get stellar parameters for each spectrum\n",
        "    teff_list = []\n",
        "    logg_list = []\n",
        "\n",
        "    # Check column names (uppercase from Gaia)\n",
        "    teff_col = 'TEFF_GSPPHOT' if 'TEFF_GSPPHOT' in sources.colnames else 'teff_gspphot'\n",
        "    logg_col = 'LOGG_GSPPHOT' if 'LOGG_GSPPHOT' in sources.colnames else 'logg_gspphot'\n",
        "    source_id_col = 'SOURCE_ID' if 'SOURCE_ID' in sources.colnames else 'source_id'\n",
        "\n",
        "    for sid in source_ids:\n",
        "        mask = sources[source_id_col] == sid\n",
        "        if np.any(mask):\n",
        "            source = sources[mask][0]\n",
        "            teff_list.append(source[teff_col])\n",
        "            logg_list.append(source[logg_col])\n",
        "        else:\n",
        "            teff_list.append(np.nan)\n",
        "            logg_list.append(np.nan)\n",
        "\n",
        "    teff_array = np.array(teff_list)\n",
        "    logg_array = np.array(logg_list)\n",
        "\n",
        "    # Analyze sparse norms vs stellar parameters\n",
        "    sparse_norms = np.linalg.norm(rpca.S, axis=1)\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "    # Sparse norm vs Teff\n",
        "    scatter1 = ax1.scatter(teff_array, sparse_norms, c=logg_array,\n",
        "                          cmap='viridis', alpha=0.6)\n",
        "    ax1.set_xlabel('Teff (K)')\n",
        "    ax1.set_ylabel('||S|| (Sparse Norm)')\n",
        "    ax1.set_title('Sparse Component vs Temperature')\n",
        "    plt.colorbar(scatter1, ax=ax1, label='log g')\n",
        "\n",
        "    # Sparse norm vs log g\n",
        "    scatter2 = ax2.scatter(logg_array, sparse_norms, c=teff_array,\n",
        "                          cmap='plasma', alpha=0.6)\n",
        "    ax2.set_xlabel('log g')\n",
        "    ax2.set_ylabel('||S|| (Sparse Norm)')\n",
        "    ax2.set_title('Sparse Component vs Surface Gravity')\n",
        "    plt.colorbar(scatter2, ax=ax2, label='Teff (K)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def export_results_for_further_analysis(results, output_prefix='rpca_gaia_rvs'):\n",
        "    \"\"\"\n",
        "    Export results in various formats for further analysis.\n",
        "    \"\"\"\n",
        "\n",
        "    # Export outlier spectra\n",
        "    outlier_table = Table()\n",
        "    outlier_table['source_id'] = results['outlier_info']['source_ids']\n",
        "    outlier_table['sparse_norm'] = results['outlier_info']['sparse_norms']\n",
        "\n",
        "    # Add stellar parameters\n",
        "    sources = results['sources']\n",
        "\n",
        "    # Check column names\n",
        "    source_id_col = 'SOURCE_ID' if 'SOURCE_ID' in sources.colnames else 'source_id'\n",
        "    teff_col = 'TEFF_GSPPHOT' if 'TEFF_GSPPHOT' in sources.colnames else 'teff_gspphot'\n",
        "    logg_col = 'LOGG_GSPPHOT' if 'LOGG_GSPPHOT' in sources.colnames else 'logg_gspphot'\n",
        "    mh_col = 'MH_GSPPHOT' if 'MH_GSPPHOT' in sources.colnames else 'mh_gspphot'\n",
        "\n",
        "    teff_list = []\n",
        "    logg_list = []\n",
        "    mh_list = []\n",
        "\n",
        "    for sid in outlier_table['source_id']:\n",
        "        mask = sources[source_id_col] == sid\n",
        "        if np.any(mask):\n",
        "            source = sources[mask][0]\n",
        "            teff_list.append(source[teff_col])\n",
        "            logg_list.append(source[logg_col])\n",
        "            mh_list.append(source[mh_col])\n",
        "        else:\n",
        "            teff_list.append(np.nan)\n",
        "            logg_list.append(np.nan)\n",
        "            mh_list.append(np.nan)\n",
        "\n",
        "    outlier_table['teff_gspphot'] = teff_list\n",
        "    outlier_table['logg_gspphot'] = logg_list\n",
        "    outlier_table['mh_gspphot'] = mh_list\n",
        "\n",
        "    # Save outlier table\n",
        "    outlier_table.write(f'{output_prefix}_outliers.fits', format='fits', overwrite=True)\n",
        "    print(f\"Outlier table saved to '{output_prefix}_outliers.fits'\")\n",
        "\n",
        "    # Save principal components\n",
        "    components = results['components']\n",
        "    pc_table = Table()\n",
        "    pc_table['wavelength'] = results['wavelength_grid']\n",
        "\n",
        "    for i in range(min(10, components['V'].shape[0])):\n",
        "        pc_table[f'PC{i+1}'] = components['V'][i, :]\n",
        "\n",
        "    pc_table.write(f'{output_prefix}_principal_components.fits', format='fits', overwrite=True)\n",
        "    print(f\"Principal components saved to '{output_prefix}_principal_components.fits'\")\n",
        "\n",
        "    print(\"\\nAll results exported successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 8. Parameter Tuning Functions\n",
        "\n",
        "def tune_lambda_parameter(Y, lambda_values=None):\n",
        "    \"\"\"\n",
        "    Tune the lambda parameter for Robust PCA.\n",
        "    \"\"\"\n",
        "\n",
        "    if lambda_values is None:\n",
        "        m, n = Y.shape\n",
        "        base_lambda = 2.0 / np.sqrt(max(m, n))\n",
        "        lambda_values = base_lambda * np.array([0.5, 0.75, 1.0, 1.25, 1.5, 2.0])\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for lam in lambda_values:\n",
        "        print(f\"\\nTesting lambda = {lam:.4f}\")\n",
        "        rpca = RobustPCA(lambda_param=lam, max_iter=400)\n",
        "        rpca.fit(Y)\n",
        "\n",
        "        result = {\n",
        "            'lambda': lam,\n",
        "            'rank': rpca.rank,\n",
        "            'sparsity': rpca.sparsity,\n",
        "            'reconstruction_error': np.linalg.norm(Y - rpca.L - rpca.S, 'fro'),\n",
        "            'sparse_norm': np.linalg.norm(rpca.S, 'fro'),\n",
        "            'low_rank_norm': np.linalg.norm(rpca.L, 'fro')\n",
        "        }\n",
        "        results.append(result)\n",
        "\n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    lambdas = [r['lambda'] for r in results]\n",
        "\n",
        "    axes[0, 0].plot(lambdas, [r['rank'] for r in results], 'bo-')\n",
        "    axes[0, 0].set_xlabel('Lambda')\n",
        "    axes[0, 0].set_ylabel('Rank')\n",
        "    axes[0, 0].set_title('Rank vs Lambda')\n",
        "    axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[0, 1].plot(lambdas, [r['sparsity'] for r in results], 'ro-')\n",
        "    axes[0, 1].set_xlabel('Lambda')\n",
        "    axes[0, 1].set_ylabel('Sparsity')\n",
        "    axes[0, 1].set_title('Sparsity vs Lambda')\n",
        "    axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 0].plot(lambdas, [r['reconstruction_error'] for r in results], 'go-')\n",
        "    axes[1, 0].set_xlabel('Lambda')\n",
        "    axes[1, 0].set_ylabel('Reconstruction Error')\n",
        "    axes[1, 0].set_title('Reconstruction Error vs Lambda')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "    axes[1, 1].plot(lambdas, [r['sparse_norm'] for r in results], 'mo-', label='||S||_F')\n",
        "    axes[1, 1].plot(lambdas, [r['low_rank_norm'] for r in results], 'co-', label='||L||_F')\n",
        "    axes[1, 1].set_xlabel('Lambda')\n",
        "    axes[1, 1].set_ylabel('Frobenius Norm')\n",
        "    axes[1, 1].set_title('Component Norms vs Lambda')\n",
        "    axes[1, 1].legend()\n",
        "    axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "bHTNzb8Am2wA"
      },
      "id": "bHTNzb8Am2wA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc23b819",
      "metadata": {
        "id": "cc23b819"
      },
      "outputs": [],
      "source": [
        "results = main_analysis()\n",
        "\n",
        "# Additional analysis can be performed here using the results dictionary\n",
        "print(\"\\nResults dictionary contains:\")\n",
        "for key in results.keys():\n",
        "    print(f\"  - {key}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8787dce0",
      "metadata": {
        "id": "8787dce0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}